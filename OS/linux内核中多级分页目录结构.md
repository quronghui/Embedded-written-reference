#  linux**内核空间**中内存分配

## 内存分配的函数

| Funcation            | 底层函数                | size     | 作用于DMA | 阻塞申请 | 内存分配位置   |
| -------------------- | ----------------------- | -------- | --------- | -------- | -------------- |
| __get_free_page()    | __alloc_pages           | 4M       | 直接      |          |                |
| kmem_cache_alloc()   | kmem_cache_create/alloc | <128kb   | 直接      |          |                |
| kmalloc()            | **同上**                | <128kb   | 直接      | 睡眠等待 | 从低端内存分配 |
| vmalloc()            |                         | <1GB     |           | 非阻塞   | 从高端内存分配 |
| dma_alloc_coherent() | __alloc_pages           | 4M       |           |          |                |
| ioremap()            |                         | 已知     |           |          |                |
| **Boot Memory**      | alloc_bootmem()         | mem=size |           |          |                |

### 内存分配的函数适用情况

1. __get_free_pages直接对页框进行操作4MB适用于分配较大量的连续物理内存
2. kmem_cache_alloc基于slab机制实现128KB适合需要频繁申请释放相同大小内存块时使用
3. kmalloc基于kmem_cache_alloc实现128KB最常见的分配方式，需要小于页框大小的内存时可以使用
4. vmalloc建立非连续物理内存到虚拟地址的映射物理不连续，适合需要大内存，但是对地址连续性没有要求的场合
5. dma_alloc_coherent基于__alloc_pages实现4MB适用于DMA操 作
6. ioremap实现已知物理地址到虚拟地址的映射适用于物理地址已知的场合，如**设备驱动**
7. alloc_bootmem在启动kernel时，预留一段内存，内核看不见小于物理内存大小，内存管理要求较高

### 如何保证申请大内存能成功

1. 在Linux内核环境下，申请大块内存的成功率随着系统运行时间的增加而减少，虽然可以通过vmalloc系列调用申请物理不连续但虚拟地址连续的内存，但毕竟其使用效率不高且在32位系统上vmalloc的内存地址空间有限。
2. 启动阶段申请内存：
   - 成功的概率也只是比较高而已，而不是100%
3. 程序特别在意申请的成功与否
   - “启动内存”Boot Memory

## linux内核中多级分页目录结构

1. Linux内核中采 用了一种同时适用于32位和64位系统的内 存分页模型
   + 对于32位系统来说，两级页表足够用了
   + 而在x86_64系 统中，用到了四级页表。

### 多级分页目录结构

+ 页全局目录(Page Global Directory)
+ 页上级目录(Page Upper Directory)
+ 页中间目录(Page Middle Directory)
+ 页表(Page Table)

1. 目录之间的包含关系
   + 全局目录包含若干页上级目录的**地址**
   + 上级目录又依次包含若干页中间目录的地址，
   + 页中间目录又包含若干页表的地址
   + 每一个页表项指向一个页框
2. Linux中采用**4KB大小**的 **页框**作为标准的内存分配单元。
   + 1M ；64KB ;4KB; 1KB

## 伙伴系统申请内核内存

1. 物理页面管理上：实现了基于**区**的伙伴系统zone based buddy system）
2. 对不同区的内存：使用单独的伙伴系统(buddy system)管理

### 伙伴系统算法 -- 分配页框

1. 内存分配不连续：导致内存块中分散了许多小块的空闲页框；
   + 即使这些页框是空闲的，其他需要分配连续页框的应用也很难得到满足
2. Linux内核中引入了伙伴系统算法(buddy system)
   + 把所有的空闲页框分组为**11个块链表**
   + 每个块链表分别包含大小(KB)：1，2，4，8，16，32，64，128，256，512和1024个连续页框的页框块
   + 最大可以申请：1024个连续页框，对应4MB大小的连续内存。
   + 每个页框块的第一个页框的**物理地址**：是该块大小的整数倍
3. 申请一个256KB大小的页框块？
   + 先从256个页框的链表中查找空闲块；
   + 如果没有，就去512个 页框的链表中查找；找到了则将页框块分为2个256KB页框的块，一个分配给应用，另外一个移到256的页框的链表中
4. 页框块在释放时
   + 会主动将两个连续的页框块合并为一个较大的页框块。

### slab分配器

1. slab分配器源于 Solaris 2.4 的 分配算法
   + 工作于**物理内存页框**分配器之上
   + 管理**特定大小**对象的缓存，进行快速而高效的内存分配。
2. slab分配器
   + 为每种使用的内核对象建立单独的缓冲区
   + 工作于**伙伴系统**之上：Linux 内核已经采用了伙伴系统管理物理内存页框；
3. 缓冲区和slab关系
   + 每种缓冲区由多个 slab 组成，每个 slab就是一组连续的物理内存页框，被划分成了固定数目的对象
   + 根据对象大小的不同，缺省情况下一个 slab 最多可以由 1024个页框构成
   + slab 中分配给对象的内存可能大于用户要求的对象实际大小，这会造成一定的 内存浪费

###  伙伴内存分配函数

1. **__get_free_pages**
   + 最原始的内存分配方式，直接从伙伴系统中获取**原始页框**
   + 返回值为第一个页框的**起始地址**。
   + 实现上只是封装了alloc_pages函 数
   
2. **kmem_cache_alloc**/kmem_cache_create
   
   + 基于slab分配器的一种内存分配方式，适用于反复分配释放同一大小内存块的场合
   + 首先用kmem_cache_create创建一个高速缓存区域，然后用kmem_cache_alloc从 该高速缓存区域中获取新的内存块
   + 用kmem_cache_create分配超过128KB：内存时使内核崩溃
   
3. **kmalloc**

4. **vmalloc**

   + 分配的一般为高端内存
   + 当内存不够的时候才分配低端内存
   + 分配的一般为大块内存

5. **dma_alloc_coherent**

   + DMA是一种硬件机制：允许外围设备和主存之间直接传输IO数据，而不需要CPU的参与，使用DMA机制能大幅提高与设备通信的 吞吐量。
   + DMA操作中，涉及到CPU高速缓 存和对应的内存数据一致性的问题，必须保证两者的数据一致
   + dma_alloc_coherent和__get_free_pages函数实现差别不大，前者实际是调用__alloc_pages函 数来分配内存，因此一次分配内存的大小限制和后者一样
   + dma_alloc_coherent函 数一次能分配的最大内存也为4M

6. **ioremap**

   + 更直接的方式：直接指定**物理起始地址**和需要**分配内存**的大小，然后将该段物理地址**映射**到内核地址空间
   + ioremap：和上面的几种内存分配方式并不太一样，**不分配一段新**的物理内存。
   + ioremap多用于设备驱动，可以让CPU直接访问外部设备的IO空间。
   + ioremap能映射的内存由原有的物理内存空间决定，所以没有进行测试。

7. **Boot Memory**

   + 如果要分配大量的连续物理内存，上述的分配函数都不能满足，就只能用比较特殊的方式

   + 在Linux内核引导阶段来**预留部分内存**

   + void* alloc_bootmem(unsigned long size)

     + 使用方法是在Linux内核引导时，调用mem_init函数之前用alloc_bootmem函数**申请指定大小的内存**
     + 如果需要在其他地方调用这块内存：将alloc_bootmem返回的内存首地址通过EXPORT_SYMBOL导出
     + 缺点：
       + 申请内存的代码必须在链接到内核中的代码里才能使用，因此必须重新编译内核
       + 内存管理系统 看不到这部分内存，需要用户自行管理
     + 重新编译内核后重启，能够访问引导时分配的内存块

   + **通过内核引导参数预留顶部内存**

     + 在Linux内核引导时，传入参数“mem=size”保留顶部的内存区间。

     + ```
       // 比如系统有256MB内 存，参数“mem=248M”会预留顶部的8MB内存
       进入系统后可以调用ioremap(0xF800000，0x800000)来申请这段内存
       ```

## DMA工作原理

1. DMA 作为**内存**与**外设**之间传输数据的方式, 不需要cpu的参与;减少CPU对快速设备入出的操作

2. 把这批数据的传输过程交由一块专用的接口卡**（DMA接口）**来控制

   - 让DMA卡代替CPU控制在快速设备与主存储器之间直接传输数据
   - CPU只须向DMA控制器下达指令，让DMA控制器来处理数据的传送，数据传送完毕再把信息反馈给CPU，这样就很大程度上减轻了CPU资源占有率。

   ![DMA.png](https://github.com/quronghui/Embedded-written-reference/blob/master/OS/photo/DMA.png)